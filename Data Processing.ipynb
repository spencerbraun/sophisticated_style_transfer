{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "from string import punctuation\n",
    "from time import sleep\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import enchant\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "d = enchant.Dict(\"en_US\")\n",
    "spacy_nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_PATH = \"/Users/spencerbraun/Documents/Stanford/CS 230 - Deep Learning/Project/CS_230_Project/data/raw/\"\n",
    "SAVE_DATA_PATH = \"/Users/spencerbraun/Documents/Stanford/CS 230 - Deep Learning/Project/CS_230_Project/data/processed/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCorpus(url):\n",
    "    content = requests.get(url).content.decode('ascii', 'ignore')\n",
    "    content_list = sent_tokenize(content.replace('\\r\\n', ' '))\n",
    "    \n",
    "    filtered_list = filterSentences(content_list)\n",
    "    \n",
    "    return filtered_list[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(data, keepnum):\n",
    "    random.seed(123)\n",
    "    \n",
    "    split1 = int(keepnum * 0.05)\n",
    "    remain = split1 % 128\n",
    "    split1 += remain\n",
    "\n",
    "    random.shuffle(data)\n",
    "    selectedData = data[0:keepnum]\n",
    "    train = selectedData[split1:]\n",
    "    test = selectedData[0:split1]\n",
    "    \n",
    "    return (train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASAP Essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aes_file = RAW_DATA_PATH + \"asap-aes/training_set_rel3.tsv\"\n",
    "aes_list = [] \n",
    "with open(aes_file, encoding='utf-8', errors='ignore') as f:\n",
    "    for line in f:\n",
    "        aes_list.append(line.strip().split('\\t'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "aes_df = pd.DataFrame(aes_list[1:], columns=aes_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [\n",
    "    'rater1_domain1', 'rater2_domain1', 'rater3_domain1', \n",
    "    'domain1_score', 'rater1_domain2', 'rater2_domain2', 'domain2_score'\n",
    "]\n",
    "\n",
    "aes_df[num_cols] = aes_df[num_cols].applymap(\n",
    "    lambda x: np.nan if (x == \"\") or (x is None) else int(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "aes_df[\"total_score\"] = (aes_df[\"domain1_score\"] + aes_df[\"domain2_score\"].fillna(aes_df[\"domain1_score\"]))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>total_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Dear local newspaper, I think effects compute...</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Dear @CAPS1 @CAPS2, I believe that using comp...</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Dear, @CAPS1 @CAPS2 @CAPS3 More and more peop...</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Dear Local Newspaper, @CAPS1 I have found tha...</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Dear @LOCATION1, I know having computers has ...</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12973</th>\n",
       "      <td>21626</td>\n",
       "      <td>8</td>\n",
       "      <td>\" In most stories mothers and daughters are ei...</td>\n",
       "      <td>35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12974</th>\n",
       "      <td>21628</td>\n",
       "      <td>8</td>\n",
       "      <td>\" I never understood the meaning laughter is t...</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12975</th>\n",
       "      <td>21629</td>\n",
       "      <td>8</td>\n",
       "      <td>\"When you laugh, is @CAPS5 out of habit, or is...</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12976</th>\n",
       "      <td>21630</td>\n",
       "      <td>8</td>\n",
       "      <td>\"                               Trippin' on fe...</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12977</th>\n",
       "      <td>21633</td>\n",
       "      <td>8</td>\n",
       "      <td>\" Many people believe that laughter can improv...</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12978 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      essay_id essay_set                                              essay  \\\n",
       "0            1         1  \"Dear local newspaper, I think effects compute...   \n",
       "1            2         1  \"Dear @CAPS1 @CAPS2, I believe that using comp...   \n",
       "2            3         1  \"Dear, @CAPS1 @CAPS2 @CAPS3 More and more peop...   \n",
       "3            4         1  \"Dear Local Newspaper, @CAPS1 I have found tha...   \n",
       "4            5         1  \"Dear @LOCATION1, I know having computers has ...   \n",
       "...        ...       ...                                                ...   \n",
       "12973    21626         8  \" In most stories mothers and daughters are ei...   \n",
       "12974    21628         8  \" I never understood the meaning laughter is t...   \n",
       "12975    21629         8  \"When you laugh, is @CAPS5 out of habit, or is...   \n",
       "12976    21630         8  \"                               Trippin' on fe...   \n",
       "12977    21633         8  \" Many people believe that laughter can improv...   \n",
       "\n",
       "       domain1_score  domain2_score  total_score  \n",
       "0                  8            NaN          8.0  \n",
       "1                  9            NaN          9.0  \n",
       "2                  7            NaN          7.0  \n",
       "3                 10            NaN         10.0  \n",
       "4                  8            NaN          8.0  \n",
       "...              ...            ...          ...  \n",
       "12973             35            NaN         35.0  \n",
       "12974             32            NaN         32.0  \n",
       "12975             40            NaN         40.0  \n",
       "12976             40            NaN         40.0  \n",
       "12977             40            NaN         40.0  \n",
       "\n",
       "[12978 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['essay_id', 'essay_set', 'essay', 'domain1_score', 'domain2_score', 'total_score']\n",
    "aes_df[cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "aes_essays = aes_df.query(\"total_score > 1\")[\"essay\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanAES(dataList):\n",
    "    newList = []\n",
    "    for sent in dataList:\n",
    "        sent = re.sub(r'(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9-_]+)', '', sent)\n",
    "        sent = sent.replace(\"\\\\\",\"\")\n",
    "        sent = sent.replace(\"\\\\'\",\"\")\n",
    "        sent = sent.strip().strip(\"'\").strip('\"')\n",
    "\n",
    "        if len(sent) < 40:\n",
    "            continue\n",
    "        if '^' in sent:\n",
    "            continue\n",
    "        \n",
    "        sent = ' '.join(sent.split())\n",
    "        sent = sent.lower()#.decode('utf8', 'ignore')\n",
    "        newList.append(sent + '\\n')\n",
    "    \n",
    "    return newList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_aes = cleanAES(aes_essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10826/10826 [01:46<00:00, 101.41it/s]\n"
     ]
    }
   ],
   "source": [
    "d_check = lambda sent: map(lambda x: d.check(x), word_tokenize(sent))\n",
    "split_aes = []\n",
    "for essay in tqdm(clean_aes):\n",
    "    split_up = sent_tokenize(essay)\n",
    "    for sent in split_up:\n",
    "        words = word_tokenize(sent)\n",
    "        if not all(list(d_check(sent))):\n",
    "            continue\n",
    "        if len(words) > 30:\n",
    "            continue\n",
    "        if len(words) < 4:\n",
    "            continue\n",
    "        split_aes.append(sent+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SAVE_DATA_PATH + \"aes.txt\", 'w') as f:\n",
    "    f.writelines(split_aes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84373"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split_aes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sophisticated Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "firstCorpus = [\n",
    "    \"http://www.gutenberg.org/cache/epub/5827/pg5827.txt\", #Russell, The Problems of Philosophy\n",
    "    \"http://www.gutenberg.org/cache/epub/15718/pg15718.txt\", #Bleyer, How To Write Special Feature Articles\n",
    "    \"https://www.gutenberg.org/files/492/492-0.txt\", #Essays in the Art of Writing, by Robert Louis\n",
    "    \"https://www.gutenberg.org/files/37090/37090-0.txt\", #Our Knowledge of the External World as a Field for Scientific Method in Philosoph, by Bertrand Russell\n",
    "    \"https://www.gutenberg.org/files/42580/42580-8.txt\", #Expository Writing, by Mervin James Curl\n",
    "    \"http://www.gutenberg.org/cache/epub/2529/pg2529.txt\", #The Analysis of Mind, by Bertrand Russell\n",
    "    \"https://www.gutenberg.org/files/38280/38280-0.txt\", #Modern Essays, by Various\n",
    "    \"https://www.gutenberg.org/files/205/205-0.txt\", #Walden, and On The Duty Of Civil Disobedience, by Henry David Thoreau\n",
    "    \"https://www.gutenberg.org/files/1022/1022-0.txt\", #Walking, by Henry David Thoreau\n",
    "    \"http://www.gutenberg.org/cache/epub/34901/pg34901.txt\",\n",
    "    \"https://www.gutenberg.org/files/98/98-0.txt\",\n",
    "    \"http://www.gutenberg.org/cache/epub/32168/pg32168.txt\",\n",
    "    \"https://www.gutenberg.org/files/766/766-0.txt\",\n",
    "    \"https://www.gutenberg.org/files/1250/1250-0.txt\",\n",
    "    \"https://www.gutenberg.org/files/140/140-0.txt\",\n",
    "    \"https://www.gutenberg.org/files/1400/1400-0.txt\",\n",
    "    \"https://www.gutenberg.org/files/215/215-0.txt\", # London, call of the wild.\n",
    "    \"http://www.gutenberg.org/cache/epub/910/pg910.txt\", #London White Fang\n",
    "    \"https://www.gutenberg.org/files/786/786-0.txt\",\n",
    "    \"http://www.gutenberg.org/cache/epub/815/pg815.txt\",\n",
    "    \"http://www.gutenberg.org/cache/epub/10378/pg10378.txt\",\n",
    "    \"http://www.gutenberg.org/cache/epub/5123/pg5123.txt\",\n",
    "    \"http://www.gutenberg.org/cache/epub/5669/pg5669.txt\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "secondCorpus = [\n",
    "    \"https://ota.bodleian.ox.ac.uk/repository/xmlui/bitstream/handle/20.500.12024/1906/cardinal-1906.txt?sequence=3&isAllowed=y\",\n",
    "    \"https://ota.bodleian.ox.ac.uk/repository/xmlui/bitstream/handle/20.500.12024/1658/WoolfWaves-1658.txt?sequence=4&isAllowed=y\",\n",
    "    \"https://ota.bodleian.ox.ac.uk/repository/xmlui/bitstream/handle/20.500.12024/0172/moderns-0172.txt?sequence=4&isAllowed=y\",\n",
    "    \"https://ota.bodleian.ox.ac.uk/repository/xmlui/bitstream/handle/20.500.12024/3246/3246.txt?sequence=8&isAllowed=y\",\n",
    "    \"https://ota.bodleian.ox.ac.uk/repository/xmlui/bitstream/handle/20.500.12024/2042/joywoman-2042.txt?sequence=4&isAllowed=y\",\n",
    "    \"https://ota.bodleian.ox.ac.uk/repository/xmlui/bitstream/handle/20.500.12024/3135/3135.txt?sequence=8&isAllowed=y\",\n",
    "    \"https://ota.bodleian.ox.ac.uk/repository/xmlui/bitstream/handle/20.500.12024/1711/wiseman-1711.txt?sequence=4&isAllowed=y\",\n",
    "    \"https://ota.bodleian.ox.ac.uk/repository/xmlui/bitstream/handle/20.500.12024/3245/3245.txt?sequence=8&isAllowed=y\",\n",
    "    \"http://www.gutenberg.org/cache/epub/5827/pg5827.txt\", #Russell, The Problems of Philosophy\n",
    "    \"http://www.gutenberg.org/cache/epub/15718/pg15718.txt\", #Bleyer, How To Write Special Feature Articles\n",
    "    \"https://www.gutenberg.org/files/492/492-0.txt\", #Essays in the Art of Writing, by Robert Louis\n",
    "    \"https://www.gutenberg.org/files/37090/37090-0.txt\", #Our Knowledge of the External World as a Field for Scientific Method in Philosoph, by Bertrand Russell\n",
    "    \"https://www.gutenberg.org/files/42580/42580-8.txt\", #Expository Writing, by Mervin James Curl\n",
    "    \"http://www.gutenberg.org/cache/epub/2529/pg2529.txt\", #The Analysis of Mind, by Bertrand Russell\n",
    "    \"https://www.gutenberg.org/files/38280/38280-0.txt\",\n",
    "    \"https://www.gutenberg.org/files/215/215-0.txt\", # London, call of the wild.\n",
    "    \"http://www.gutenberg.org/cache/epub/910/pg910.txt\",\n",
    "    \"https://www.gutenberg.org/files/25110/25110-0.txt\",\n",
    "    \"http://www.gutenberg.org/cache/epub/32168/pg32168.txt\",\n",
    "    \"http://www.gutenberg.org/cache/epub/16712/pg16712.txt\",\n",
    "    \"http://www.gutenberg.org/cache/epub/7514/pg7514.txt\",\n",
    "    \"http://www.gutenberg.org/cache/epub/18477/pg18477.txt\",\n",
    "    \"http://www.gutenberg.org/cache/epub/5669/pg5669.txt\",\n",
    "    \"http://www.gutenberg.org/cache/epub/5123/pg5123.txt\",\n",
    "    \"http://www.gutenberg.org/cache/epub/10378/pg10378.txt\",\n",
    "    \"https://www.gutenberg.org/files/140/140-0.txt\",\n",
    "    \"http://www.gutenberg.org/cache/epub/44082/pg44082.txt\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterSentences(sentList):\n",
    "    filteredList = []\n",
    "    for sent in sentList:\n",
    "        sent = sent.replace(\"\\\\\",\"\")\n",
    "        sent = sent.replace(\"\\\\'\",\"\")\n",
    "\n",
    "\n",
    "        if len(sent) < 40:\n",
    "            continue\n",
    "        if '^' in sent:\n",
    "            continue\n",
    "        if bool(re.search(r'\\d', sent)):\n",
    "            continue\n",
    "        if bool(re.search(r\"\\b[A-Z][A-Z]+\\b\", sent)):\n",
    "            continue\n",
    "        if bool(re.search(r'\\\"', sent)):\n",
    "            continue\n",
    "        if bool(re.search(r'_', sent)):\n",
    "            continue\n",
    "\n",
    "        sent = sent.strip()\n",
    "        sent = sent.lower()\n",
    "        sent = ' '.join(sent.split())\n",
    "        filteredList.append(sent + '\\n')\n",
    "\n",
    "    return filteredList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "allGuten = []\n",
    "for url in firstCorpus:\n",
    "    allGuten.append(readCorpus(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69955"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(x) for x in allGuten])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allSophs = [y for x in allGuten for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(SAVE_DATA_PATH + \"allsophs.txt\", 'w') as f:\n",
    "    f.writelines(allSophs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(SAVE_DATA_PATH + \"allsophs.txt\", 'r') as f:\n",
    "    allSophs = f.read_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "allSecondCorpus = []\n",
    "for url in secondCorpus:\n",
    "    allSecondCorpus.append(readCorpus(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51291"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(x) for x in allSecondCorpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allSophs = [y for x in allSecondCorpus for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(SAVE_DATA_PATH + \"soph_2.txt\", 'w') as f:\n",
    "    f.writelines(allSophs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Corpus without Puctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "punctSoph = [y for x in allSecondCorpus for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "allSophs = list(map(removePunc, punctSoph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SAVE_DATA_PATH + \"KMW_essays.txt\", 'r') as f:\n",
    "    kmw = f.readlines()\n",
    "with open(SAVE_DATA_PATH + \"aes.txt\", 'r') as f:\n",
    "    split_aes = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allnaive = kmw + split_aes[0:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allnaive = list(map(removePunc, allnaive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hewlett ASAP + Sophisticated with Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process ASAP tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aes_file = RAW_DATA_PATH + \"asap-aes/training_set_rel3.tsv\"\n",
    "aes_list = [] \n",
    "with open(aes_file, encoding='utf-8', errors='ignore') as f:\n",
    "    for line in f:\n",
    "        aes_list.append(line.strip().split('\\t'))\n",
    "        \n",
    "aes_df = pd.DataFrame(aes_list[1:], columns=aes_list[0])    \n",
    "num_cols = [\n",
    "    'rater1_domain1', 'rater2_domain1', 'rater3_domain1', \n",
    "    'domain1_score', 'rater1_domain2', 'rater2_domain2','domain2_score'\n",
    "]\n",
    "\n",
    "aes_df[num_cols] = aes_df[num_cols].applymap(\n",
    "    lambda x: np.nan if (x == \"\") or (x is None) else int(x)\n",
    ")\n",
    "aes_df[\"total_score\"] = (1/2)*(\n",
    "    aes_df[\"domain1_score\"] + \n",
    "    aes_df[\"domain2_score\"].fillna(aes_df[\"domain1_score\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "aes_essays = aes_df.query(\"total_score > 1\")[\"essay\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_regex = r'(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[0-9]+)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list = []\n",
    "new_strings = []\n",
    "for string in aes_essays:\n",
    "    matches = re.findall(token_regex, string)\n",
    "    token_list.extend(list(set(matches)))\n",
    "    \n",
    "    string = string.replace('@' , '')\n",
    "    \n",
    "    replacement = {x: \"<\" + re.sub('[0-9]', '', x) + \">\" for x in matches}\n",
    "    for match in matches:\n",
    "        string = string.replace(match, replacement[match])\n",
    "    new_strings.append(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_tokens = list(set([re.sub('[0-9]', '', x) for x in token_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MONTH',\n",
       " 'PERCENT',\n",
       " 'STATE',\n",
       " 'CAPS',\n",
       " 'NUM',\n",
       " 'TIME',\n",
       " 'CITY',\n",
       " 'LOCATION',\n",
       " 'PERSON',\n",
       " 'MONEY',\n",
       " 'DR',\n",
       " 'ORGANIZATION',\n",
       " 'DATE']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "general_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanAES(dataList):\n",
    "    newList = []\n",
    "    for sent in dataList:\n",
    "        \n",
    "        sent = sent.replace(\"\\\\'\",\"\")\n",
    "        sent = sent.strip().strip(\"'\").strip('\"')\n",
    "        sent = sent.replace(\"'\", \"\")\n",
    "        sent = sent.replace('\"', \"\")\n",
    "        sent = re.sub('([.,!?()])', r' \\1 ', sent)\n",
    "        sent = re.sub('\\s{2,}', ' ', sent)\n",
    "        sent = sent.replace(\">s\", \">\")\n",
    "        sent = sent.strip()\n",
    "        \n",
    "\n",
    "        if len(sent) < 40:\n",
    "            continue\n",
    "        if '^' in sent:\n",
    "            continue\n",
    "        \n",
    "        sent = ' '.join(sent.split())\n",
    "        sent = sent.lower()\n",
    "        newList.append(sent + '\\n')\n",
    "    \n",
    "    return newList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_aes = cleanAES(new_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b796100cd914790ad301e5502a08eb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10826), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "d_check = lambda words: map(lambda x: d.check(x), words)\n",
    "split_aes = []\n",
    "for essay in tqdm.notebook.tqdm(clean_aes):\n",
    "    split_up = sent_tokenize(essay)\n",
    "    for sent in split_up:\n",
    "        words_ex_tokens = [x for x in sent.split() if x.upper().strip('<').strip('>') not in general_tokens]\n",
    "        words = word_tokenize(' '.join(words_ex_tokens))\n",
    "        if not all(list(d_check(words))):\n",
    "            continue\n",
    "        if len(words) > 30:\n",
    "            continue\n",
    "        if len(words) < 4:\n",
    "            continue\n",
    "        split_aes.append(sent+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59886"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split_aes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gutenberg + Oxford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "taggedCorpus = [\n",
    "    \"http://www.gutenberg.org/cache/epub/5827/pg5827.txt\", #Russell, The Problems of Philosophy\n",
    "    \"http://www.gutenberg.org/cache/epub/15718/pg15718.txt\", #Bleyer, How To Write Special Feature Articles\n",
    "    \"https://www.gutenberg.org/files/492/492-0.txt\", #Essays in the Art of Writing, by Robert Louis\n",
    "    \"https://www.gutenberg.org/files/37090/37090-0.txt\", #Our Knowledge of the External World as a Field for Scientific Method in Philosoph, by Bertrand Russell\n",
    "    \"https://www.gutenberg.org/files/42580/42580-8.txt\", #Expository Writing, by Mervin James Curl\n",
    "    \"http://www.gutenberg.org/cache/epub/2529/pg2529.txt\", #The Analysis of Mind, by Bertrand Russell\n",
    "    \"https://www.gutenberg.org/files/38280/38280-0.txt\", #Modern Essays, by Various\n",
    "    \"https://www.gutenberg.org/files/205/205-0.txt\", #Walden, and On The Duty Of Civil Disobedience, by Henry David Thoreau\n",
    "    \"https://www.gutenberg.org/files/1022/1022-0.txt\", #Walking, by Henry David Thoreau\n",
    "    \"http://www.gutenberg.org/cache/epub/34901/pg34901.txt\",\n",
    "    \"https://www.gutenberg.org/files/98/98-0.txt\",\n",
    "    \"http://www.gutenberg.org/cache/epub/32168/pg32168.txt\",\n",
    "    \"https://www.gutenberg.org/files/1250/1250-0.txt\",\n",
    "    \"https://www.gutenberg.org/files/140/140-0.txt\",\n",
    "    \"https://www.gutenberg.org/files/215/215-0.txt\", # London, call of the wild.\n",
    "    \"http://www.gutenberg.org/cache/epub/910/pg910.txt\", #London White Fang\n",
    "    \"http://www.gutenberg.org/cache/epub/10378/pg10378.txt\",\n",
    "    \"http://www.gutenberg.org/cache/epub/5123/pg5123.txt\",\n",
    "    \"http://www.gutenberg.org/cache/epub/5669/pg5669.txt\",\n",
    "    \"https://ota.bodleian.ox.ac.uk/repository/xmlui/bitstream/handle/20.500.12024/1906/cardinal-1906.txt?sequence=3&isAllowed=y\",\n",
    "    \"https://ota.bodleian.ox.ac.uk/repository/xmlui/bitstream/handle/20.500.12024/1658/WoolfWaves-1658.txt?sequence=4&isAllowed=y\",\n",
    "    \"https://ota.bodleian.ox.ac.uk/repository/xmlui/bitstream/handle/20.500.12024/0172/moderns-0172.txt?sequence=4&isAllowed=y\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterSentences(sentList):\n",
    "    filteredList = []\n",
    "    for sent in sentList:\n",
    "        sent = sent.replace(\"\\\\\",\"\")\n",
    "        sent = sent.replace(\"\\\\'\",\"\")\n",
    "\n",
    "\n",
    "        if len(sent) < 40:\n",
    "            continue\n",
    "        if '^' in sent:\n",
    "            continue\n",
    "        if bool(re.search(r\"\\b[A-Z][A-Z]+\\b\", sent)):\n",
    "            continue\n",
    "        if bool(re.search(r'_', sent)):\n",
    "            continue\n",
    "\n",
    "        sent = sent.strip()\n",
    "        sent = ' '.join(sent.split())\n",
    "        filteredList.append(sent + '\\n')\n",
    "\n",
    "    return filteredList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "taggedTexts = []\n",
    "for url in taggedCorpus:\n",
    "    taggedTexts.append(readCorpus(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50860"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(x) for x in taggedTexts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTaggedSophs = [y for x in taggedTexts for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StanfordNERTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "jar = \"stanford-ner-2018-10-16/stanford-ner-3.9.2.jar\"\n",
    "model = \"stanford-ner-2018-10-16/classifiers/english.muc.7class.distsim.crf.ser.gz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = StanfordNERTagger(model, jar) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_HASH = {}\n",
    "\n",
    "def tagSentence(sent):\n",
    "    tokenize = word_tokenize(sent)\n",
    "    tagged = st.tag(tokenize)\n",
    "    \n",
    "    tokens = dict([x for x in tagged if x[1] != 'O'])\n",
    "    tokens = {x: \"<\" + y  + \">\" for x,y in tokens.items() }\n",
    "    new_sent = [tokens.get(x, x) for x in tokenize]\n",
    "    for word, repl in tokens.items():\n",
    "        TAG_HASH[word] = repl\n",
    "    \n",
    "    return(' '.join(new_sent) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_write_path = SAVE_DATA_PATH + 'tagged_data/'\n",
    "batches = [x for x in range(len(allTaggedSophs)) if x % 1000 == 0]\n",
    "batches.append(len(allTaggedSophs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm.notebook.tqdm(range(len(batches)-1)):\n",
    "    \n",
    "    start = batches[i]\n",
    "    stop = batches[i+1]\n",
    "    batch = allTaggedSophs[start:stop]\n",
    "\n",
    "    stanfordTaggedSophs = []\n",
    "    for sent in tqdm.notebook.tqdm(batch):\n",
    "        stanfordTaggedSophs.append(tagSentence(sent))\n",
    "        \n",
    "    with open(tagged_write_path + f'batch{i}', 'w') as f:\n",
    "        f.writelines(stanfordTaggedSophs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Descartes': '<PERSON>',\n",
       " 'Detroit': '<LOCATION>',\n",
       " 'United': '<LOCATION>',\n",
       " 'States': '<LOCATION>',\n",
       " 'Lawrence': '<PERSON>',\n",
       " 'Kansas': '<ORGANIZATION>',\n",
       " 'Blackmar': '<PERSON>',\n",
       " 'the': '<ORGANIZATION>',\n",
       " 'University': '<ORGANIZATION>',\n",
       " 'of': '<ORGANIZATION>',\n",
       " 'Leibniz': '<PERSON>',\n",
       " 'Berkeley': '<LOCATION>',\n",
       " 'Bishop': '<LOCATION>',\n",
       " 'China': '<LOCATION>',\n",
       " 'Bismarck': '<PERSON>',\n",
       " 'Germany': '<LOCATION>',\n",
       " 'London': '<LOCATION>',\n",
       " 'England': '<LOCATION>',\n",
       " 'Europe': '<LOCATION>',\n",
       " 'Earth': '<LOCATION>'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TAG_HASH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spacy NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://spacy.io/api/annotation#named-entities\n",
    "spacy2stanford = {\n",
    "    'NORP': 'CAPS',\n",
    "    'FAC': 'LOCATION',\n",
    "    'ORG': 'ORGANIZATION',\n",
    "    'GPE': 'STATE',\n",
    "    'LOC': 'LOCATION',\n",
    "    'PRODUCT': 'CAPS',\n",
    "    'EVENT': 'CAPS',\n",
    "    'WORK_OF_ART': 'CAPS',\n",
    "    'LAW': 'CAPS',\n",
    "    'LANGUAGE': 'CAPS',\n",
    "    'QUANTITY': 'NUM',\n",
    "    'ORDINAL': 'NUM',\n",
    "    'CARDINAL': 'NUM'   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_write_path = SAVE_DATA_PATH + 'tagged_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPACY_TOKENS = {}\n",
    "def spacyTagger(sent):\n",
    "    tokenize = word_tokenize(sent)\n",
    "    document = spacy_nlp(sent)\n",
    "    token_map = {}\n",
    "    for element in document.ents:\n",
    "        label = spacy2stanford.get(str(element.label_), str(element.label_))\n",
    "        SPACY_TOKENS[str(element)] = label\n",
    "        token_map[str(element)] = \"<\" + label + \">\"\n",
    "    \n",
    "    new_sent = [token_map.get(x, x) for x in tokenize]\n",
    "    \n",
    "    return(' '.join(new_sent) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "220713a9a3c54f99951b1f75f9e1d3e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50860), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spacyTaggedSophs = []\n",
    "for sent in tqdm.notebook.tqdm(allTaggedSophs):\n",
    "    spacyTaggedSophs.append(spacyTagger(sent))\n",
    "\n",
    "with open(spacy_write_path + f'spacyTaggedSophs.txt', 'w') as f:\n",
    "    f.writelines(spacyTaggedSophs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(spacy_write_path + 'spacyTaggedSophs.txt') as f:\n",
    "    tagged = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = list(map(str.lower, tagged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "keepshort = []\n",
    "for sent in tagged:\n",
    "    words = len(sent.split(' '))\n",
    "    if words <= 30:\n",
    "        keepshort.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32155"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keepshort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1674\n",
      "30426\n"
     ]
    }
   ],
   "source": [
    "\n",
    "keepnum = 32100\n",
    "sophstrain, sophtest = splitData(keepshort, keepnum)\n",
    "\n",
    "print(len(sophtest))\n",
    "print(len(sophstrain))\n",
    "\n",
    "with open(SAVE_DATA_PATH + \"soph_test_tagged.txt\", 'w') as f:\n",
    "    f.writelines(sophtest)\n",
    "with open(SAVE_DATA_PATH + \"soph_train_tagged.txt\", 'w') as f:\n",
    "    f.writelines(sophstrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "allnaive = [x for x in split_aes if len(x) > 20]\n",
    "naiveshort = []\n",
    "for sent in allnaive:\n",
    "    words = len(sent.split(' '))\n",
    "    if words <= 30:\n",
    "        naiveshort.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58784"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(naiveshort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1674\n",
      "30426\n",
      "32100\n"
     ]
    }
   ],
   "source": [
    "naivetrain, naivetest = splitData(naiveshort, keepnum)\n",
    "\n",
    "print(len(naivetest))\n",
    "print(len(naivetrain))\n",
    "\n",
    "with open(SAVE_DATA_PATH + \"naive_test_tagged.txt\", 'w') as f:\n",
    "    f.writelines(naivetest)\n",
    "with open(SAVE_DATA_PATH + \"naive_train_tagged.txt\", 'w') as f:\n",
    "    f.writelines(naivetrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagged without Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePunc(sent):\n",
    "    punct = string.punctuation.replace('<', '').replace('>', '')\n",
    "    sent = re.sub('['+punct+']', '', sent)\n",
    "    sent = ' '.join(sent.split())\n",
    "    return(sent + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_nopunct = list(map(removePunc, tagged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_aes_nopunct = list(map(removePunc, split_aes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1674\n",
      "30426\n"
     ]
    }
   ],
   "source": [
    "short_soph_tagged = []\n",
    "for sent in tagged_nopunct:\n",
    "    words = len(sent.split(' '))\n",
    "    if words <= 30:\n",
    "        short_soph_tagged.append(sent)\n",
    "\n",
    "\n",
    "sophstrain, sophtest = splitData(short_soph_tagged, keepnum)\n",
    "\n",
    "print(len(sophtest))\n",
    "print(len(sophstrain))\n",
    "\n",
    "with open(SAVE_DATA_PATH + \"soph_test_tagged_nopunct.txt\", 'w') as f:\n",
    "    f.writelines(sophtest)\n",
    "with open(SAVE_DATA_PATH + \"soph_train_tagged_nopunct.txt\", 'w') as f:\n",
    "    f.writelines(sophstrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1674\n",
      "30426\n",
      "32100\n"
     ]
    }
   ],
   "source": [
    "allnaive = [x for x in split_aes_nopunct if len(x) > 20]\n",
    "naiveshort_tag_np = []\n",
    "for sent in allnaive:\n",
    "    words = len(sent.split(' '))\n",
    "    if words <= 30:\n",
    "        naiveshort_tag_np.append(sent)\n",
    "        \n",
    "naivetrain, naivetest = splitData(naiveshort_tag_np, keepnum)\n",
    "\n",
    "with open(SAVE_DATA_PATH + \"naive_test_tagged_nopunct.txt\", 'w') as f:\n",
    "    f.writelines(naivetest)\n",
    "with open(SAVE_DATA_PATH + \"naive_train_tagged_nopunct.txt\", 'w') as f:\n",
    "    f.writelines(naivetrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Kids Way Essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for item in text:\n",
    "    data.append(item.get_text().split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_links = \"https://www.mykidsway.com/essays/page/{}/\"\n",
    "\n",
    "all_essays = req = requests.get(\"https://www.mykidsway.com/essays/\")\n",
    "essay_html = BeautifulSoup(all_essays.content, 'html.parser')\n",
    "divs = essay_html.find_all(\"div\", class_=\"hovereffect\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = []\n",
    "for content in divs:\n",
    "    all_links.append(content.find(\"a\").get(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,20):\n",
    "    new_page = paginated_links.format(str(i))\n",
    "    \n",
    "    all_essays = req = requests.get(new_page)\n",
    "    essay_html = BeautifulSoup(all_essays.content, 'html.parser')\n",
    "    divs = essay_html.find_all(\"div\", class_=\"hovereffect\")\n",
    "    \n",
    "    for content in divs:\n",
    "        all_links.append(content.find(\"a\").get(\"href\"))\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getText(link):\n",
    "    \n",
    "    req = requests.get(link)\n",
    "    soup = BeautifulSoup(req.content, 'html.parser')\n",
    "    text = soup.find_all(\"span\", itemprop=\"description\")\n",
    "    \n",
    "    data = []\n",
    "    for item in text:\n",
    "        split_text = item.get_text().split('\\n')\n",
    "        total_len = sum([len(x) for x in split_text])\n",
    "        if total_len > 2000:\n",
    "            print(\"skipping \", link)\n",
    "            continue\n",
    "            \n",
    "        for sentence in split_text:\n",
    "            data.append(sentence)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_sentences = []\n",
    "for link in set(all_links):\n",
    "    print(link)\n",
    "    data_list = getText(link)\n",
    "    for sentence in data_list:\n",
    "        all_sentences.append(sentence)\n",
    "        \n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cleanKMW(data_list):\n",
    "    newList = []\n",
    "    for sent in data_list:\n",
    "        if len(sent) < 40:\n",
    "            continue\n",
    "        if '^' in sent:\n",
    "            continue\n",
    "        if bool(re.search(r'\\d', sent)):\n",
    "            continue\n",
    "            \n",
    "        sent = sent.lower()\n",
    "        newList.append(sent + '\\n')\n",
    "    \n",
    "    return newList\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleanedKMW = cleanKMW(all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "reordered = []\n",
    "for sent in cleanedKMW:\n",
    "    split_sent = sent_tokenize(sent.strip())\n",
    "    for sentence in split_sent:\n",
    "        if (len(word_tokenize(sentence)) > 20) or (len(word_tokenize(sentence)) < 4):\n",
    "            continue\n",
    "        reordered.append(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SAVE_DATA_PATH + \"KMW_essays.txt\", 'w') as f:\n",
    "    f.writelines(reordered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Saving Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def limitLength(dataList, maxlen):\n",
    "    keepshort = []\n",
    "    for sent in dataList:\n",
    "        words = len(word_tokenize(sent))\n",
    "        if words <= maxlen:\n",
    "            keepshort.append(sent)\n",
    "            \n",
    "    return keepshort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1676\n",
      "31732\n"
     ]
    }
   ],
   "source": [
    "def writeOut(data, fileroot, maxlen):\n",
    "    keepshort = limitLength(dataList, maxlen)\n",
    "    train, test = splitData(keepshort, keepnum)\n",
    "\n",
    "    with open(SAVE_DATA_PATH + f\"test_{fileroot}.txt\", 'w') as f:\n",
    "        f.writelines(test)\n",
    "    with open(SAVE_DATA_PATH + f\"train_{fileroot}.txt\", 'w') as f:\n",
    "        f.writelines(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(SAVE_DATA_PATH + \"KMW_essays.txt\", 'r') as f:\n",
    "    kmw = f.readlines()\n",
    "with open(SAVE_DATA_PATH + \"aes.txt\", 'r') as f:\n",
    "    split_aes = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "allnaive = kmw + split_aes[0:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1676\n",
      "31732\n",
      "33408\n"
     ]
    }
   ],
   "source": [
    "writeOut(allnaive, \"naive_3\", 35)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
